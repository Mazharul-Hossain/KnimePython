{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic - Machine Learning from Disaster\n",
    "\n",
    "It is the [Titanic](https://www.kaggle.com/c/titanic/) competition from Kaggle. Download all the data from kaggle and put it in <i>titanic</i> folder.\n",
    "\n",
    "This notebook uses [tfgpuenv](../environments/tfgpuenv.yml) for running. Take a look in [README](../environments/README.md) for details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv(\"titanic/train.csv\")\n",
    "# Survived column is not at the end\n",
    "training['Survived'] = training.pop('Survived')\n",
    "training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Encoding data\n",
    "\n",
    "[How to handle categorical data in scikit with pandas](https://www.kaggle.com/getting-started/27270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler, LabelEncoder, OrdinalEncoder, MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titan_oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "# https://www.geeksforgeeks.org/standardscaler-minmaxscaler-and-robustscaler-techniques-ml/\n",
    "titan_sc = MinMaxScaler(feature_range = (0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(X, train=True):\n",
    "    global titan_oe, norm_sc\n",
    "\n",
    "    def is_alone(a, b):\n",
    "        if a + b == 0:\n",
    "            return 1\n",
    "        return 0\n",
    "    \n",
    "    def is_minor(s, a):\n",
    "        if a < 17:\n",
    "            return 0\n",
    "        elif s == \"female\":\n",
    "            return 1\n",
    "        else: return 2\n",
    "\n",
    "\n",
    "    X['is_m'] = X.apply(lambda x: is_minor(x.Sex, x.Age), axis=1)\n",
    "    X['is_a'] = X.apply(lambda x: is_alone(x.SibSp, x.Parch), axis=1)\n",
    "\n",
    "    if train:\n",
    "        titan_sc.fit(X[[\"Age\", \"Fare\"]])\n",
    "    X[[\"Age\", \"Fare\"]] = titan_sc.transform(X[[\"Age\", \"Fare\"]])\n",
    "    \n",
    "    if train:\n",
    "        titan_oe.fit(X[[\"Sex\", \"Embarked\"]])\n",
    "    X[[\"Sex\", \"Embarked\"]] = titan_oe.transform(X[[\"Sex\", \"Embarked\"]])\n",
    "    X[\"Embarked\"] = X[\"Embarked\"] + 1\n",
    "    \n",
    "    \n",
    "    X = X.drop(columns=[\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"])\n",
    "    \n",
    "    # if train:\n",
    "    #     titan_sc.fit(X)\n",
    "    # return pd.DataFrame(data=titan_sc.transform(X), columns=X.columns)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "e_X = encode_data(training.copy().iloc[:, :-1])\n",
    "\n",
    "e_X['Survived'] = training['Survived']\n",
    "e_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# profile = ProfileReport(e_X)\n",
    "# profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Remove NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Amount of missing values in each column: ')\n",
    "e_X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_mean_a = 0\n",
    "age_mean = 0\n",
    "fare_mean = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan(X, train=True):\n",
    "    global age_mean, age_mean_a, fare_mean\n",
    "    \n",
    "    if train:\n",
    "        age_mean_a = X[(X[\"is_a\"]==1)][\"Age\"].mean()\n",
    "        age_mean = X[(X[\"is_a\"]==0)][\"Age\"].mean()\n",
    "    \n",
    "    mask = X[\"Age\"].isna()\n",
    "    X.loc[mask, \"Age\"] = np.where(X.loc[mask, \"is_a\"].eq(1), age_mean_a, age_mean)\n",
    "    \n",
    "    if train:\n",
    "        fare_mean = X[\"Fare\"].mean()\n",
    "    X[\"Fare\"].fillna(fare_mean, inplace=True)\n",
    "    \n",
    "    X[\"Embarked\"].fillna(0, inplace=True)\n",
    "    X[[\"Sex\", \"Embarked\"]] = X[[\"Sex\", \"Embarked\"]].astype(int)\n",
    "    # X[[\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"is_m\", \"is_a\", \"Survived\"]] = X[[\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"is_m\", \"is_a\", \"Survived\"]].astype(int)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "e_X = remove_nan(e_X)\n",
    "e_X[e_X.isnull().any(axis=1)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (e_X < 0).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# profile = ProfileReport(e_X)\n",
    "# profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 data, class division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = e_X.iloc[:, :-1], e_X.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Principal Component Analysis(PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components = len(X.columns)) \n",
    "\n",
    "principalComponents = pca.fit_transform(X.values)\n",
    "ev=pca.explained_variance_ratio_\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(np.cumsum(np.append([0], ev)))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "plt.grid(True, alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_com = 6 # len(X.columns)\n",
    "pca = PCA(n_components=n_com)\n",
    "\n",
    "X_pca = pca.fit_transform(X.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Check all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.executable)\n",
    "# print(sys.version)\n",
    "# print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! conda install -c anaconda tensorflow-gpu\n",
    "# ! conda list tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://stackoverflow.com/a/66027093/2049763\n",
    "# ! pip uninstall -y tensorflow-estimator\n",
    "# ! pip uninstall -y tensorboard\n",
    "\n",
    "# ! conda install scipy=1.4.1 \n",
    "# ! conda install -y -c anaconda tensorflow-estimator=2.6\n",
    "# ! conda install -y -c conda-forge tensorboard=2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout \n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.columns)\n",
    "X_new = X[['Pclass', 'Sex', 'Age', 'Fare', 'is_m', 'is_a']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "model_m = Sequential([\n",
    "    Dense(units = 16, input_shape= (len(X_new.columns), ), activation = 'relu'),\n",
    "    Dropout(rate = 0.1),\n",
    "    Dense(units = 32, activation = 'relu'),\n",
    "    Dropout(rate = 0.1),\n",
    "    Dense(units = 64, activation = 'relu'),\n",
    "    Dropout(rate = 0.1),\n",
    "    Dense(units = 8, activation = 'relu'),\n",
    "    Dropout(rate = 0.1),\n",
    "    Dense(units = 2, activation = 'sigmoid')  # softmax \n",
    "])\n",
    "model_m.summary()\n",
    "\n",
    "# Model compilation\n",
    "model_m.compile(optimizer=Adam(learning_rate = 0.001),  # SGD(lr=0.01, momentum=0.95) \n",
    "              loss = 'sparse_categorical_crossentropy', \n",
    "              metrics = ['accuracy'] \n",
    "             )\n",
    "\n",
    "es = EarlyStopping(monitor='loss', mode='auto', verbose=1, patience=5)\n",
    "# tqdm_callback = tfa.callbacks.TQDMProgressBar()\n",
    "\n",
    "# Model Training and Validation\n",
    "model_m.fit(x = X_new.values, y = y.values, \n",
    "          batch_size= 8, epochs = 50, \n",
    "        #   validation_split= 0.20, \n",
    "          shuffle = True, verbose = 1, callbacks=[es] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = pd.read_csv(\"titanic/test.csv\")\n",
    "df = testing.filter([\"PassengerId\"], axis=1)\n",
    "\n",
    "testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = encode_data(testing, train=False)\n",
    "testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = remove_nan(testing, train=False)\n",
    "testing[testing.isnull().any(axis=1)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XT_new = testing[['Pclass', 'Sex', 'Age', 'Fare', 'is_m', 'is_a']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = model_m.predict(x=XT_new.values, batch_size=8, verbose=0)\n",
    "df[\"Survived\"] = np.argmax(y_p, axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p1 = model_m.predict(x=X_new.values, batch_size=8, verbose=0)\n",
    "y_p1 = np.argmax(y_p1, axis=1)\n",
    "\n",
    "# y_p2 = model_m.predict(x=XT_new.values, batch_size=8, verbose=0)\n",
    "y_p2 = np.argmax(y_p, axis=1)\n",
    "\n",
    "for f, r in zip([\"titanic/train.csv\", \"titanic/submission.csv\"], [y_p1, y_p2] ):\n",
    "    result = pd.read_csv(f)\n",
    "    # result.head()\n",
    "    # result.groupby([\"Survived\", \"Prediction\"]).size()\n",
    "\n",
    "    report = classification_report(result[\"Survived\"].values, r)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"titanic/submission.csv\", index=False)\n",
    "# ! kaggle competitions submit -c titanic -f titanic/submission.csv -m \"Keras DNN\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed0a59a19b8c63da79c742e4264cc12e7d2293400754d148e0142cdf47e43bbb"
  },
  "kernelspec": {
   "display_name": "Python [conda env:hsi_env] *",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
