{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic - Machine Learning from Disaster\n",
    "\n",
    "It is the [Titanic](https://www.kaggle.com/c/titanic/) competition from Kaggle. Download all the data from kaggle and put it in <i>titanic</i> folder.\n",
    "\n",
    "This notebook uses feature selection techniques. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv(\"titanic/train.csv\")\n",
    "# Survived column is not at the end\n",
    "training['Survived'] = training.pop('Survived')\n",
    "training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Encoding data\n",
    "\n",
    "[How to handle categorical data in scikit with pandas](https://www.kaggle.com/getting-started/27270)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler, LabelEncoder, OrdinalEncoder, MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titan_oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "# https://www.geeksforgeeks.org/standardscaler-minmaxscaler-and-robustscaler-techniques-ml/\n",
    "titan_sc = MinMaxScaler(feature_range = (0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(X, train=True):\n",
    "    global titan_oe, norm_sc\n",
    "\n",
    "    def is_alone(a, b):\n",
    "        if a + b == 0:\n",
    "            return 1\n",
    "        return 0\n",
    "    \n",
    "    def is_minor(s, a):\n",
    "        if a < 17:\n",
    "            return 0\n",
    "        elif s == \"female\":\n",
    "            return 1\n",
    "        else: return 2\n",
    "\n",
    "\n",
    "    X['is_m'] = X.apply(lambda x: is_minor(x.Sex, x.Age), axis=1)\n",
    "    X['is_a'] = X.apply(lambda x: is_alone(x.SibSp, x.Parch), axis=1)\n",
    "\n",
    "    if train:\n",
    "        titan_sc.fit(X[[\"Age\", \"Fare\"]])\n",
    "    X[[\"Age\", \"Fare\"]] = titan_sc.transform(X[[\"Age\", \"Fare\"]])\n",
    "    \n",
    "    if train:\n",
    "        titan_oe.fit(X[[\"Sex\", \"Embarked\"]])\n",
    "    X[[\"Sex\", \"Embarked\"]] = titan_oe.transform(X[[\"Sex\", \"Embarked\"]])\n",
    "    X[\"Embarked\"] = X[\"Embarked\"] + 1\n",
    "    \n",
    "    \n",
    "    X = X.drop(columns=[\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"])\n",
    "    \n",
    "    # if train:\n",
    "    #     titan_sc.fit(X)\n",
    "    # return pd.DataFrame(data=titan_sc.transform(X), columns=X.columns)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "e_X = encode_data(training.copy().iloc[:, :-1])\n",
    "\n",
    "e_X['Survived'] = training['Survived']\n",
    "e_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# profile = ProfileReport(e_X)\n",
    "# profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Remove NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Amount of missing values in each column: ')\n",
    "e_X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_mean_a = 0\n",
    "age_mean = 0\n",
    "fare_mean = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan(X, train=True):\n",
    "    global age_mean, age_mean_a, fare_mean\n",
    "    \n",
    "    if train:\n",
    "        age_mean_a = X[(X[\"is_a\"]==1)][\"Age\"].mean()\n",
    "        age_mean = X[(X[\"is_a\"]==0)][\"Age\"].mean()\n",
    "    \n",
    "    mask = X[\"Age\"].isna()\n",
    "    X.loc[mask, \"Age\"] = np.where(X.loc[mask, \"is_a\"].eq(1), age_mean_a, age_mean)\n",
    "    \n",
    "    if train:\n",
    "        fare_mean = X[\"Fare\"].mean()\n",
    "    X[\"Fare\"].fillna(fare_mean, inplace=True)\n",
    "    \n",
    "    X[\"Embarked\"].fillna(0, inplace=True)\n",
    "    X[[\"Sex\", \"Embarked\"]] = X[[\"Sex\", \"Embarked\"]].astype(int)\n",
    "    # X[[\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"is_m\", \"is_a\", \"Survived\"]] = X[[\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"is_m\", \"is_a\", \"Survived\"]].astype(int)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "e_X = remove_nan(e_X)\n",
    "e_X[e_X.isnull().any(axis=1)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (e_X < 0).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# profile = ProfileReport(e_X)\n",
    "# profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 data, class division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = e_X.iloc[:, :-1], e_X.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = MinMaxScaler(feature_range = (0,1))\n",
    "X_scaled = ss.fit_transform(X)\n",
    "# X_test_scaled = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply SelectKBest class to extract top 10 best features\n",
    "bestfeatures = SelectKBest( score_func=chi2, k=len(X.columns) )\n",
    "fit = bestfeatures.fit(X_scaled,y)\n",
    "\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(10,'Score'))  #print 10 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_scaled, y)\n",
    "\n",
    "importances = pd.DataFrame(data={\n",
    "    'Attribute': X.columns,\n",
    "    'Importance': model.coef_[0]\n",
    "})\n",
    "importances = importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "plt.bar(x=importances['Attribute'], height=importances['Importance'])\n",
    "plt.title('Feature importances obtained from coefficients', size=20)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X_scaled, y)\n",
    " \n",
    "#use inbuilt class feature_importances of tree based classifiers\n",
    "for c, f in zip(X.columns, model.feature_importances_):\n",
    "    print('{:10s}: {:.2f}'.format(c, f*100) )\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest( len(X.columns) ).plot(kind='bar')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# count examples in each class\n",
    "counter = Counter(y)\n",
    "\n",
    "# estimate scale_pos_weight value\n",
    "sp_weight = counter[0] / counter[1]\n",
    "print('Estimate: %.3f' % sp_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "XGB_model = XGBClassifier(scale_pos_weight=sp_weight)\n",
    "XGB_model.fit(X_scaled, y)\n",
    "importances = pd.DataFrame(data={\n",
    "    'Attribute': X.columns,\n",
    "    'Importance': XGB_model.feature_importances_\n",
    "})\n",
    "importances = importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "plt.bar(x=importances['Attribute'], height=importances['Importance'])\n",
    "plt.title('Feature importances obtained from coefficients', size=20)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(18,12))\n",
    "fig1.subplots_adjust(wspace=.05, hspace=0.25)\n",
    "\n",
    "for i, f in enumerate([\"weight\", \"gain\", \"cover\", \"total_gain\", \"total_cover\"], start=1):\n",
    "    ax = fig1.add_subplot(2, 3, i)\n",
    "\n",
    "    h = [v for k, v in XGB_model.get_booster().get_score(importance_type= f).items()]\n",
    "    ax.bar(x=X.columns, height=h)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.title(\" \".join(f.split(\"_\") ).capitalize(), size=20)\n",
    "\n",
    "ax = fig1.add_subplot(2, 3, 6)\n",
    "\n",
    "h = XGB_model.feature_importances_\n",
    "ax.bar(x=X.columns, height=h)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.title(\"Importance\", size=20)\n",
    "\n",
    "fig1.suptitle('Feature importance obtained from coefficients', size=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix with Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = e_X.corr()\n",
    "top_corr_features = corrmat.index\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "#plot heat map\n",
    "g=sns.heatmap(e_X[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "# Draw the scatter plot\n",
    "plt.bar(corrmat['Survived'].index[:-1], corrmat['Survived'][:-1])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed0a59a19b8c63da79c742e4264cc12e7d2293400754d148e0142cdf47e43bbb"
  },
  "kernelspec": {
   "display_name": "Python [conda env:hsi_env] *",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
